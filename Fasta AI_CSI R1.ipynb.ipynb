{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24662d2f-2d87-43c9-afac-55390e0cd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìì Notebook: AIV HA Country Analysis + GPT-3.5 / GPT-4o Auto Switch + Rate Limit Handling + Progress Bar + Autosave + Resume Support\n",
    "# --- üõ†Ô∏è 1. Load Required Packages ---\n",
    "import importlib.util\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import zipfile\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import site\n",
    "import logging\n",
    "from packaging import version as pkg_version\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='aiv_analysis.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Define required third-party packages with their pip installation names and version ranges\n",
    "required_packages = {\n",
    "    'pandas': 'pandas>=2.0.0,<3.0.0',\n",
    "    'matplotlib': 'matplotlib>=3.7.0',\n",
    "    'geopandas': 'geopandas>=0.13.0',\n",
    "    'plotly': 'plotly>=5.14.0',\n",
    "    'openai': 'openai>=1.0.0',\n",
    "    'tqdm': 'tqdm>=4.65.0',\n",
    "    'charset_normalizer': 'charset-normalizer>=3.3.0'  # Added for Stage 3 encoding detection\n",
    "}\n",
    "\n",
    "def check_write_permission():\n",
    "    \"\"\"\n",
    "    Check if there is write permission to the site-packages directory.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if permission is granted, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        site_packages = site.getsitepackages()[0]\n",
    "        return os.access(site_packages, os.W_OK)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Error checking write permission: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_package(pkg, retries=3):\n",
    "    \"\"\"\n",
    "    Install a specified package with retry mechanism.\n",
    "\n",
    "    Args:\n",
    "        pkg (str): Package name (including version range).\n",
    "        retries (int): Maximum number of retries.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if installation succeeds, False otherwise.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "            logging.info(f\"Successfully installed package: {pkg}\")\n",
    "            print(f\"‚úÖ Successfully installed {pkg}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logging.error(f\"Failed to install {pkg}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                wait_time = 5 * (attempt + 1)\n",
    "                print(f\"‚è≥ Failed to install {pkg}, retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to install {pkg}: {e}\")\n",
    "                return False\n",
    "\n",
    "def check_and_install_packages(packages):\n",
    "    \"\"\"\n",
    "    Check and install missing packages.\n",
    "\n",
    "    Args:\n",
    "        packages (dict): Mapping of module names to pip installation names/version ranges.\n",
    "    \"\"\"\n",
    "    missing_packages = []\n",
    "    for module_name, pip_name in packages.items():\n",
    "        try:\n",
    "            if importlib.util.find_spec(module_name) is None:\n",
    "                missing_packages.append(pip_name)\n",
    "            else:\n",
    "                # Check if installed version meets requirements\n",
    "                module = importlib.import_module(module_name)\n",
    "                installed_version = pkg_version.parse(getattr(module, '__version__', '0.0.0'))\n",
    "                required_version = pip_name.split('>=')[1].split(',')[0]\n",
    "                if installed_version < pkg_version.parse(required_version):\n",
    "                    missing_packages.append(pip_name)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking {module_name}: {e}\")\n",
    "            missing_packages.append(pip_name)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(f\"‚ùå The following required packages are missing or have incompatible versions: {', '.join(missing_packages)}\")\n",
    "        \n",
    "        # Check write permission and provide guidance\n",
    "        if not check_write_permission():\n",
    "            print(\"‚ö†Ô∏è No write permission to site-packages. Consider using a virtual environment or --user installation:\")\n",
    "            print(f\"pip install --user {' '.join(missing_packages)}\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Default to installing in non-interactive environments\n",
    "        install_prompt = os.getenv('AUTO_INSTALL', 'y') if not sys.stdin.isatty() else \\\n",
    "                         input(\"Would you like to install them now? (y/n): \").strip().lower()\n",
    "        \n",
    "        if install_prompt == 'y':\n",
    "            print(\"üì¶ Installing missing packages...\")\n",
    "            for pkg in missing_packages:\n",
    "                if not install_package(pkg):\n",
    "                    print(\"‚ùó Please install the missing packages manually using the following command:\")\n",
    "                    print(f\"pip install {' '.join(missing_packages)}\")\n",
    "                    sys.exit(1)\n",
    "        else:\n",
    "            print(\"‚ùó Please install the missing packages manually using the following command:\")\n",
    "            print(f\"pip install {' '.join(missing_packages)}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(\"‚úÖ All required packages are already installed and meet version requirements.\")\n",
    "\n",
    "# Generate requirements.txt\n",
    "def generate_requirements_file(packages, filename='requirements.txt'):\n",
    "    \"\"\"\n",
    "    Generate a requirements.txt file listing all dependencies.\n",
    "\n",
    "    Args:\n",
    "        packages (dict): Mapping of module names to pip installation names/version ranges.\n",
    "        filename (str): Path to the requirements.txt file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for _, pip_name in packages.items():\n",
    "                f.write(f\"{pip_name}\\n\")\n",
    "        print(f\"üìù Generated {filename}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to generate {filename}: {e}\")\n",
    "        print(f\"‚ùå Failed to generate {filename}: {e}\")\n",
    "\n",
    "# Execute package check and generate requirements.txt\n",
    "check_and_install_packages(required_packages)\n",
    "generate_requirements_file(required_packages)\n",
    "\n",
    "# Import third-party packages\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import geopandas as gpd\n",
    "    import plotly.express as px\n",
    "    import openai\n",
    "    from tqdm import tqdm\n",
    "    import charset_normalizer\n",
    "    print(\"‚úÖ Successfully imported all third-party packages.\")\n",
    "except ImportError as e:\n",
    "    logging.error(f\"Failed to import third-party packages: {e}\")\n",
    "    print(f\"‚ùå Failed to import third-party packages: {e}\")\n",
    "    print(\"‚ùó Please ensure all packages are correctly installed and try again.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- 2. User configuration ---\n",
    "fasta_folder = \".\"\n",
    "fasta_files = sorted(glob.glob(os.path.join(fasta_folder, \"*.fasta\")), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "if fasta_files:\n",
    "    fasta_path = fasta_files[0]\n",
    "    print(f\"‚úÖ Auto-selected the latest FASTA fileÔºö{fasta_path}\")\n",
    "else:\n",
    "     raise FileNotFoundError(\"‚ùó No .fasta files found. Please check the directory.\")\n",
    "world_shapefile_path = \"ne_110m_admin_0_countries.shp\"\n",
    "location_to_country_path = \"location_to_country_AI.json\"\n",
    "\n",
    "# --- 3. Initialize Location-Country Mapping ---\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import shutil\n",
    "import glob\n",
    "import charset_normalizer\n",
    "\n",
    "def backup_json_file(file_path, max_backups=5):\n",
    "    \"\"\"\n",
    "    Create a backup of the JSON file with a timestamped filename.\n",
    "    Limit the number of backup files to max_backups.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file to back up.\n",
    "        max_backups (int): Maximum number of backup files to retain.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: Path to the backup file or None if no backup was created.\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_path = f\"location_to_country_backup_{timestamp}.json\"\n",
    "        try:\n",
    "            shutil.copy(file_path, backup_path)\n",
    "            print(f\"üìù Backed up existing mapping to {backup_path}\")\n",
    "            \n",
    "            # Manage backup files to keep only the latest max_backups\n",
    "            backups = sorted(glob.glob(\"location_to_country_backup_*.json\"))\n",
    "            while len(backups) > max_backups:\n",
    "                oldest_backup = backups.pop(0)\n",
    "                os.remove(oldest_backup)\n",
    "                print(f\"üóëÔ∏è Removed old backup: {oldest_backup}\")\n",
    "            \n",
    "            return backup_path\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create backup of {file_path}: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def read_file_with_encoding(file_path):\n",
    "    \"\"\"\n",
    "    Read a file with automatic encoding detection.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Decoded file content.\n",
    "    \n",
    "    Raises:\n",
    "        IOError: If the file cannot be read.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        result = charset_normalizer.detect(raw_data)\n",
    "        encoding = result['encoding'] if result['encoding'] else 'utf-8'\n",
    "        return raw_data.decode(encoding)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"‚ùå Failed to read {file_path}: {e}\")\n",
    "\n",
    "def load_json_with_recovery(file_path, backup_path=None):\n",
    "    \"\"\"\n",
    "    Load a JSON file with recovery from a backup if the file is corrupted.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file.\n",
    "        backup_path (str, optional): Path to a backup file to restore from.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Loaded JSON data or an empty dict if loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = read_file_with_encoding(file_path)\n",
    "        data = json.loads(content)\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"Invalid JSON format: Expected a dictionary\")\n",
    "        return data\n",
    "    except (json.JSONDecodeError, ValueError, IOError) as e:\n",
    "        print(f\"‚ùå Error loading {file_path}: {e}\")\n",
    "        if backup_path and os.path.exists(backup_path):\n",
    "            print(f\"üîÑ Attempting to restore from {backup_path}\")\n",
    "            try:\n",
    "                content = read_file_with_encoding(backup_path)\n",
    "                data = json.loads(content)\n",
    "                if not isinstance(data, dict):\n",
    "                    raise ValueError(\"Invalid JSON format in backup: Expected a dictionary\")\n",
    "                # Restore the original file from backup\n",
    "                shutil.copy(backup_path, file_path)\n",
    "                print(f\"‚úÖ Restored {file_path} from {backup_path}\")\n",
    "                return data\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to restore from {backup_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def initialize_location_mapping(location_to_country_path=\"location_to_country_AI.json\"):\n",
    "    \"\"\"\n",
    "    Initialize the location-to-country mapping by loading or creating a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        location_to_country_path (str): Path to the JSON file storing location-to-country mappings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Loaded or initialized location-to-country mappings.\n",
    "    \"\"\"\n",
    "    # Create a backup of the existing JSON file\n",
    "    backup_path = backup_json_file(location_to_country_path)\n",
    "    \n",
    "    # Load or initialize the mapping\n",
    "    if os.path.exists(location_to_country_path):\n",
    "        location_to_country = load_json_with_recovery(location_to_country_path, backup_path)\n",
    "        print(f\"‚úÖ Loaded {len(location_to_country)} location-to-country entries.\")\n",
    "    else:\n",
    "        location_to_country = {}\n",
    "        print(\"‚ö° location_to_country_AI.json not found. Creating an empty dictionary.\")\n",
    "        try:\n",
    "            with open(location_to_country_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(location_to_country, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"‚úÖ Created and saved empty {location_to_country_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create {location_to_country_path}: {e}\")\n",
    "    \n",
    "    return location_to_country\n",
    "\n",
    "# Initialize the location-to-country mapping\n",
    "location_to_country_path = \"location_to_country_AI.json\"\n",
    "location_to_country = initialize_location_mapping(location_to_country_path)\n",
    "\n",
    "# --- 4. Parse location from FASTA headers ---\n",
    "with open(fasta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Extract only header lines (those starting with '>')\n",
    "headers = [line.strip() for line in lines if line.startswith(\">\")]\n",
    "location_list = []\n",
    "fail_count = 0\n",
    "fail_headers = []  # ‚¨ÖÔ∏è New: record headers from which location extraction failed\n",
    "\n",
    "\n",
    "# Use regex to extract location nameÔºàThe substring between the second and third slashesÔºâ\n",
    "for header in headers:\n",
    "    match = re.search(r\"A/[^/]+/([^/]+)/\", header)\n",
    "    if match:\n",
    "        location = match.group(1).strip()\n",
    "        location_list.append(location)\n",
    "    else:\n",
    "        fail_count += 1\n",
    "        fail_headers.append(header)\n",
    "\n",
    "print(f\"üìã Total raw FASTA header sequences: {len(headers)}. Successfully extracted locations: {len(location_list)}. Error: {fail_count}.\")\n",
    "\n",
    "# üì¢ Display list of headers that caused extraction errors\n",
    "if fail_headers:\n",
    "    print(\"‚ö†Ô∏è The following virus strain headers could not be processed for location extraction:\")\n",
    "    for fh in fail_headers:\n",
    "        print(f\"  - {fh}\")\n",
    "\n",
    "# --- 5. Load OpenAI API Key ---\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"‚ùó Environment variable OPENAI_API_KEY not found. Please set your API key first!\")\n",
    "\n",
    "# --- 6. Ask user which ChatGPT model to use today ---\n",
    "print(\"üß† Please select the ChatGPT model to use today:\")\n",
    "print(\"1. gpt-3.5-turbo-1106 (Economy Version)\")\n",
    "print(\"2. gpt-4o (Enhanced Version)\")\n",
    "\n",
    "while True:\n",
    "    choice = input(\"Enter 1 or 2 (default is gpt-4o): \").strip()\n",
    "    if choice in [\"1\", \"2\", \"\"]:\n",
    "        break\n",
    "    print(\"‚ùå Invalid choice. Please enter 1 or 2.\")\n",
    "\n",
    "if choice == \"1\":\n",
    "    model_name = \"gpt-3.5-turbo-1106\"\n",
    "    print(\"‚úÖ You selected ‚ûî gpt-3.5-turbo-1106 (Economy Version)\")\n",
    "else:\n",
    "    model_name = \"gpt-4o\"\n",
    "    print(\"‚úÖ You selected ‚ûî gpt-4o (Enhanced Version)\")\n",
    "\n",
    "# --- 7. Automatically detect openai-python version and set API call method ---\n",
    "try:\n",
    "    from importlib.metadata import version\n",
    "except ImportError:\n",
    "    from pkg_resources import get_distribution as version\n",
    "\n",
    "openai_version = version(\"openai\")\n",
    "print(f\"üîé Current openai-python version: {openai_version}\")\n",
    "major_version = int(openai_version.split(\".\")[0])\n",
    "\n",
    "# --- 8. Define ChatGPT batch inference function (rate-limit resilient version) ---\n",
    "if major_version >= 1:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=openai_api_key)\n",
    "else:\n",
    "    openai.api_key = openai_api_key\n",
    "\n",
    "def ask_batch(batch):\n",
    "    prompt = (\n",
    "    \"\"\"Determine the country corresponding to each of the following locations.\n",
    "Location names may include: \n",
    "1.ISO 3166-1 alpha-3 country codes\n",
    "2.Names of states, provinces, or counties in English, including common abbreviations\n",
    "3.Major city names and their abbreviations\n",
    "Remove any leading hyphens (-) or whitespace from each location before processing.\n",
    "Use the format ‚ÄúLocation:Country‚Äù, one entry per line.\n",
    "Apply reasonable inference for misspellings and romanization variants, but avoid over-guessing.\n",
    "If the country cannot be clearly determined, assign ‚ÄúOther‚Äù.\n",
    "Treat ‚ÄúTaiwan‚Äù as a sovereign and independent country.\n",
    "Country names must strictly follow ISO 3166-1 short English names‚Äîno abbreviations, notes, or extra comments.\n",
    "Standardize the following aliases:\n",
    "USA, US, United States ‚Üí United States of America\n",
    "UK ‚Üí United Kingdom\n",
    "Korea, South Korea ‚Üí Korea, Republic of\n",
    "North Korea: Korea ‚Üí Democratic People's Republic of\"\"\"\n",
    ")\n",
    "    prompt += \"\\n\".join(batch)\n",
    "    prompt += \"\\n\\nPlease output only the results in the required format:\"\n",
    "\n",
    "    tries = 0\n",
    "    while tries < 5:\n",
    "        try:\n",
    "            if major_version >= 1:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a highly professional and erudite geographer.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0\n",
    "                )\n",
    "                reply = response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a highly professional and erudite geographer.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0\n",
    "                )\n",
    "                reply = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "            return reply\n",
    "        except Exception as e:\n",
    "            err_msg = str(e)\n",
    "            if \"rate limit\" in err_msg.lower() or \"Rate limit reached\" in err_msg:\n",
    "                print(f\"‚è≥ Rate limit encountered. Waiting 20 seconds before retrying... (Attempt {tries + 1})\")\n",
    "                time.sleep(20)\n",
    "            else:\n",
    "                print(f\"‚ö° Batch request failed on attempt {tries + 1}: {e}\")\n",
    "                time.sleep(10)\n",
    "            tries += 1\n",
    "    return None\n",
    "\n",
    "# --- 9. Load location list ---\n",
    "# location_list = [...]  # <-- Remember to insert your list of locations here\n",
    "\n",
    "# Count the occurrences of each location\n",
    "location_counter = Counter(location_list)\n",
    "total_samples = sum(location_counter.values())\n",
    "\n",
    "# Convert to DataFrame, sort by count, and save as CSV\n",
    "location_df = pd.DataFrame(location_counter.items(), columns=[\"Location\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "location_df.to_csv(\"location_list.csv\", index=False)\n",
    "print(f\"üìÑ Location list saved to location_list.csv (Total: {len(location_df)} records)\")\n",
    "\n",
    "# --- 10. Load existing inference records and failed attempts ---\n",
    "failed_locations_path = \"failed_locations.json\"\n",
    "\n",
    "try:\n",
    "    with open(location_to_country_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        location_to_country = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    location_to_country = {}\n",
    "\n",
    "try:\n",
    "    with open(failed_locations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        failed_locations = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    failed_locations = []\n",
    "\n",
    "# --- 11. Batch inference module (with 2 rounds of retry for failed cases) ---\n",
    "def batch_guess_and_update_with_fail_record(\n",
    "    location_to_country,\n",
    "    unknown_locations,\n",
    "    location_to_country_path,\n",
    "    failed_locations_path=\"failed_locations.json\",\n",
    "    batch_size=20,\n",
    "    save_every_batch=True,\n",
    "    retry_failed_rounds=2,\n",
    "    max_total_attempts=5,\n",
    "    print_summary=True\n",
    "):\n",
    "    def single_batch_process(batch, batch_idx, total_batches):\n",
    "        batch_success = 0\n",
    "        batch_fail = 0\n",
    "        batch_failed = []\n",
    "\n",
    "        if print_summary:\n",
    "            print(f\"\\nüîµ [Batch {batch_idx}/{total_batches}] Started ‚ûî {len(batch)} locations\")\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        reply = ask_batch(batch)\n",
    "\n",
    "        if reply is None:\n",
    "            if print_summary:\n",
    "                print(f\"‚ùó Batch {batch_idx} completely failed ‚Äî all locations moved to failed list.\")\n",
    "            batch_failed.extend(batch)\n",
    "        else:\n",
    "            lines = reply.split(\"\\n\")\n",
    "            parsed = set()\n",
    "\n",
    "            for line in lines:\n",
    "                if \":\" in line:\n",
    "                    loc, country = line.split(\":\", 1)\n",
    "                    loc = loc.strip()\n",
    "                    country = country.strip()\n",
    "                    if loc and country:\n",
    "                        location_to_country[loc] = country\n",
    "                        parsed.add(loc)\n",
    "\n",
    "            missing = set(batch) - parsed\n",
    "            batch_failed.extend(list(missing))\n",
    "\n",
    "            batch_success = len(parsed)\n",
    "            batch_fail = len(missing)\n",
    "\n",
    "        elapsed = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        avg = elapsed / len(batch) if batch else 0\n",
    "\n",
    "        if print_summary:\n",
    "            print(f\"‚úÖ Batch {batch_idx} done in {elapsed:.1f}s (avg: {avg:.2f}s/loc)\")\n",
    "            print(f\"üéØ Success: {batch_success}, Failures: {batch_fail}\")\n",
    "\n",
    "        if save_every_batch:\n",
    "            with open(location_to_country_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(location_to_country, f, ensure_ascii=False, indent=4)\n",
    "            with open(failed_locations_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(batch_failed, f, ensure_ascii=False, indent=4)\n",
    "            if print_summary:\n",
    "                print(f\"üìù Checkpoint saved ‚Äî {len(location_to_country)} mapped, {len(batch_failed)} failed\")\n",
    "\n",
    "        time.sleep(3)\n",
    "        return batch_failed\n",
    "\n",
    "    # --- Initial batch split ---\n",
    "    total = len(unknown_locations)\n",
    "    batches = [unknown_locations[i:i + batch_size] for i in range(0, total, batch_size)]\n",
    "    if print_summary:\n",
    "        print(f\"üöÄ Starting inference: {total} locations ‚Üí {len(batches)} batches of {batch_size}\")\n",
    "\n",
    "    all_failed_locations = []\n",
    "\n",
    "    # --- Main pass ---\n",
    "    for batch_idx, batch in enumerate(tqdm(batches, desc=\"üß† Running inference...\", ncols=100), 1):\n",
    "        failed = single_batch_process(batch, batch_idx, len(batches))\n",
    "        all_failed_locations.extend(failed)\n",
    "\n",
    "    # --- Retry rounds ---\n",
    "    attempt = 1\n",
    "    while attempt <= max_total_attempts and all_failed_locations and attempt <= retry_failed_rounds:\n",
    "        if print_summary:\n",
    "            print(f\"\\nüîÑ Retry round {attempt} ‚Äî {len(all_failed_locations)} unresolved locations\")\n",
    "\n",
    "        retry_batches = [all_failed_locations[i:i + batch_size] for i in range(0, len(all_failed_locations), batch_size)]\n",
    "        new_failed = []\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(retry_batches, desc=f\"‚ôªÔ∏è Retry {attempt}\", ncols=100), 1):\n",
    "            failed = single_batch_process(batch, batch_idx, len(retry_batches))\n",
    "            new_failed.extend(failed)\n",
    "\n",
    "        all_failed_locations = new_failed\n",
    "        attempt += 1\n",
    "\n",
    "    # --- Final save ---\n",
    "    with open(location_to_country_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(location_to_country, f, ensure_ascii=False, indent=4)\n",
    "    with open(failed_locations_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_failed_locations, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    if print_summary:\n",
    "        print(f\"\\nüèÅ Inference complete. Final unresolved: {len(all_failed_locations)}\")\n",
    "\n",
    "    return location_to_country, all_failed_locations\n",
    "\n",
    "# --- 12. Identify unfinished locations and continue inference ---\n",
    "unknown_locations = [\n",
    "    loc for loc in location_counter\n",
    "    if (location_to_country.get(loc) is None) or (location_to_country.get(loc) == \"Other\")\n",
    "]\n",
    "unknown_locations = list(set(unknown_locations + failed_locations))\n",
    "\n",
    "print(f\"üîé {len(unknown_locations)} locations still require inference!\")\n",
    "\n",
    "if unknown_locations:\n",
    "    location_to_country, failed_locations = batch_guess_and_update_with_fail_record(\n",
    "        location_to_country,\n",
    "        unknown_locations,\n",
    "        location_to_country_path,\n",
    "        failed_locations_path=failed_locations_path,\n",
    "        batch_size=20,\n",
    "        save_every_batch=True,\n",
    "        retry_failed_rounds=2\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úÖ All locations have been successfully inferred. Great job!\")\n",
    "\n",
    "# --- 13. Country-level sample statistics ---\n",
    "# Initialize counters for countries and unmatched locations\n",
    "country_counter = defaultdict(int)  # Tracks sample counts per country\n",
    "other_locations = defaultdict(int)  # Tracks unmatched locations and their counts\n",
    "\n",
    "# Aggregate sample counts by country\n",
    "# For each location, retrieve its corresponding country from location_to_country\n",
    "# If no country is found, classify as \"Other\" and track in other_locations\n",
    "for loc, count in location_counter.items():\n",
    "    country = location_to_country.get(loc, \"Other\")\n",
    "    if country == \"Other\":\n",
    "        other_locations[loc] += count\n",
    "    country_counter[country] += count\n",
    "\n",
    "# Create a DataFrame for country statistics, sorted by sample count in descending order\n",
    "country_df = pd.DataFrame(\n",
    "    sorted(country_counter.items(), key=lambda x: x[1], reverse=True),\n",
    "    columns=[\"Country\", \"Sample_Count\"]\n",
    ")\n",
    "\n",
    "# Verify that the sum of country sample counts matches the total samples\n",
    "assert country_df[\"Sample_Count\"].sum() == total_samples, \\\n",
    "    f\"Inconsistent total sample count! Computed: {country_df['Sample_Count'].sum()}, Expected: {total_samples}\"\n",
    "\n",
    "# Save country statistics to CSV\n",
    "country_df.to_csv(\"country_stat.csv\", index=False)\n",
    "print(\"üìÑ Country statistics saved to country_stat.csv\")\n",
    "\n",
    "# Create and save statistics for unmatched \"Other\" locations\n",
    "if other_locations:\n",
    "    # Create a DataFrame for unmatched locations, sorted by count in descending order\n",
    "    other_df = pd.DataFrame(\n",
    "        sorted(other_locations.items(), key=lambda x: x[1], reverse=True),\n",
    "        columns=[\"Location\", \"Sample_Count\"]\n",
    "    )\n",
    "    # Save to CSV for further analysis\n",
    "    other_df.to_csv(\"other_locations.csv\", index=False)\n",
    "    print(\"üìÑ Unmatched location statistics saved to other_locations.csv\")\n",
    "else:\n",
    "    # Report if all locations were successfully corresponded to countries\n",
    "    print(\"‚úÖ No unmatched locations; all locations successfully corresponded to countries\")\n",
    "\n",
    "# Final success report\n",
    "print(f\"üèÅ All country statistics completed! Total samples extracted: {total_samples}, \"\n",
    "      f\"Sum of country counts: {country_df['Sample_Count'].sum()} - fully matched ‚úÖ\")\n",
    "\n",
    "# Inference summary report\n",
    "success_samples = total_samples - sum(other_locations.values())\n",
    "fail_samples = sum(other_locations.values())\n",
    "\n",
    "# Report the number of samples successfully corresponded to countries\n",
    "print(f\"üåê Successfully identified countries for {success_samples} virus isolates using ISO-3166-1 and AI.\")\n",
    "# Report the number of samples classified as \"Other\"\n",
    "print(f\"{fail_samples} isolates could not be resolved and are temporarily classified as 'Other'.\")\n",
    "# List up to 20 unmatched locations with their counts\n",
    "print(\"Top 20 unmatched locations and their counts are listed below:\")\n",
    "\n",
    "if other_locations:\n",
    "    # Display the top 20 unmatched locations, sorted by count\n",
    "    for loc, count in sorted(other_locations.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "        print(f\"  - {loc}: {count} isolates\")\n",
    "else:\n",
    "    print(\"  No unmatched locations\")\n",
    "\n",
    "# --- 14. Generate bar chart of top 20 countries by sample count ---\n",
    "country_df = pd.read_csv(\"country_stat.csv\")\n",
    "\n",
    "# Select the top 20 countries\n",
    "top20_countries = country_df.head(20)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.barh(top20_countries['Country'], top20_countries['Sample_Count'], color='steelblue')\n",
    "plt.xlabel('Sample Count', fontsize=13)\n",
    "plt.title('Top 20 Countries by Sample Count', fontsize=15, pad=15)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.gca().invert_yaxis()  # Highest bar on top\n",
    "\n",
    "# Annotate each bar with its value\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + (0.01 * top20_countries['Sample_Count'].max()),  # Slightly offset to the right\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             f'{int(width)}',\n",
    "             va='center', ha='left', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save high-resolution PNG\n",
    "plt.savefig(\"top20_countries_sample_count.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "print(\"üñºÔ∏è Bar chart saved with annotated sample counts: top20_countries_sample_count.png\")\n",
    "\n",
    "# --- 15. Plot static world map ---\n",
    "world = gpd.read_file(world_shapefile_path)\n",
    "world[\"Samples\"] = world[\"NAME\"].map(lambda x: country_counter.get(x, 0))\n",
    "\n",
    "# üß† Custom title prefix\n",
    "title_prefix = \"H5 AIV Global Surveillance\"  # üî• You can freely modify this prefix\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "world.plot(column=\"Samples\", cmap=\"Blues\", linewidth=0.8, ax=ax, edgecolor=\"0.8\", legend=True)\n",
    "plt.title(f\"{title_prefix} Samples by Country (Static Map)\", fontsize=20)\n",
    "plt.savefig(\"static_map.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# --- 16. Plot interactive choropleth map ---\n",
    "# üß† Custom title prefix\n",
    "title_prefix = \"H5 AIV Global Surveillance\"  # üî• You can freely change this prefix\n",
    "\n",
    "fig = px.choropleth(\n",
    "    country_df,\n",
    "    locations=\"Country\",\n",
    "    locationmode=\"country names\",\n",
    "    color=\"Sample_Count\",\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    title=f\"{title_prefix} Samples by Country (Interactive Map)\"\n",
    ")\n",
    "fig.write_html(\"interactive_map.html\")\n",
    "fig.show()\n",
    "\n",
    "# --- 17. Print suggested manual additions for location_to_country ---\n",
    "print(\"\\nüìã Suggested additions to manually update 'location_to_country':\\n\")\n",
    "\n",
    "if 'other_df' not in locals():\n",
    "    # If other_df doesn't exist, create it from location_to_country\n",
    "    other_df = pd.DataFrame([\n",
    "        {\"Location\": loc, \"AI_Guess\": \"Other\"}  # Default AI guess is 'Other'; you can customize it\n",
    "        for loc in location_to_country\n",
    "        if location_to_country[loc] == \"Other\"\n",
    "    ])\n",
    "else:\n",
    "    # If other_df exists, ensure it has the AI_Guess column\n",
    "    if \"AI_Guess\" not in other_df.columns:\n",
    "        other_df[\"AI_Guess\"] = \"Other\"\n",
    "\n",
    "# Check if other_df is empty; skip iteration if it is\n",
    "if not other_df.empty:\n",
    "    for _, row in other_df.iterrows():\n",
    "        print(f'    \"{row[\"Location\"]}: \"{row[\"AI_Guess\"]}\",')\n",
    "else:\n",
    "    print(\"  No unmatched locations to suggest for manual addition.\")\n",
    "\n",
    "# --- 18. One-click packaging of output files ---\n",
    "def pack_all_results(output_folder=\".\"):\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"Results_{timestamp}.zip\"\n",
    "    zip_path = os.path.join(output_folder, zip_filename)\n",
    "\n",
    "    files_to_zip = [\n",
    "        \"location_list.csv\",\n",
    "        \"country_stat.csv\",\n",
    "        \"other_locations.csv\",\n",
    "        \"static_map.png\",\n",
    "        \"interactive_map.html\",\n",
    "        \"location_to_country_AI.json\"\n",
    "        \"failed_locations.json\"\n",
    "    ]\n",
    "    files_to_zip += glob.glob(\"location_to_country_backup_*.json\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in files_to_zip:\n",
    "            if os.path.exists(file):\n",
    "                zipf.write(file)\n",
    "\n",
    "    print(f\"\\nüì¶ Packaging complete: {zip_path}\")\n",
    "\n",
    "pack_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e58c3-fa01-46f8-a01d-160859e86b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py312]",
   "language": "python",
   "name": "conda-env-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
